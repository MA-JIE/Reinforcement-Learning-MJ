策略迭代
=========
*求解强化学习中的值函数可以看做为一个动态规划(Dynamic Progamming)问题. DP的核心思想是使用价值函数来结构化地组织对最优策略的搜索.<br>
*我们最终的目标是得到能够满足价值函数的贝尔曼最优方程: 最优策略下,各个状态的价值一定等于这个状态下最优动作的期望回报.
![最优贝尔曼方程](https://github.com/MA-JIE/Reinforcement-Learning-MJ/blob/master/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/images/bellman_opti.png)

策略评估(预测)
-----------
*对于一给定的策略 𝜋,我们期望计算其价值函数, 在DP中,这被成为策略评估或预测问题.对于:<br>
![](https://github.com/MA-JIE/Reinforcement-Learning-MJ/blob/master/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/images/bellman_equation.png) <br>
如果环境的动态特性完全已知,共有n个状态,那么上式我们可以通过求解n个线性方程将每个状态的值函数求解出来,但太过于麻烦,可使用迭代法解决此问题.
![迭代法](https://github.com/MA-JIE/Reinforcement-Learning-MJ/blob/master/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/images/bellman_iteration.png)<br>
迭代的过程就是值函数不断更新的过程, 对于每个状态 s: 按照之前给定的策略,得到所有可能的单步转移之后的即时收益(R)和s的每个后继状态的'旧'的价值(v(s')).<br> 
其伪代码如下:
![策略评估](https://github.com/MA-JIE/Reinforcement-Learning-MJ/blob/master/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/images/policy_iteration.png)<br>

策略改进
-----------

策略迭代
----------
