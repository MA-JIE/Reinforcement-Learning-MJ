策略迭代
=========
* 求解强化学习中的值函数可以看做为一个动态规划(Dynamic Progamming)问题. DP的核心思想是使用价值函数来结构化地组织对最优策略的搜索.<br>
 我们最终的目标是得到能够满足价值函数的贝尔曼最优方程: 最优策略下,各个状态的价值一定等于这个状态下最优动作的期望回报.
![最优贝尔曼方程](https://github.com/MA-JIE/Reinforcement-Learning-MJ/blob/master/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/images/bellman_opti.png)

策略评估(预测)
-----------
* 对于一给定的策略 𝜋,我们期望计算其价值函数, 在DP中,这被成为策略评估或预测问题.对于:<br>
![](https://github.com/MA-JIE/Reinforcement-Learning-MJ/blob/master/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/images/bellman_equation.png) <br>
 如果环境的动态特性完全已知,共有n个状态,那么上式我们可以通过求解n个线性方程将每个状态的值函数求解出来,但太过于麻烦,可使用迭代法解决此问题.<br>
![迭代法](https://github.com/MA-JIE/Reinforcement-Learning-MJ/blob/master/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/images/bellman_iteration.png)<br>
* 迭代的过程就是值函数不断更新的过程, 对于每个状态 s: 按照之前给定的策略,得到所有可能的单步转移之后的即时收益(R)和s的每个后继状态的'旧'的价值(v(s')).<br> 
其伪代码如下:
![策略评估](https://github.com/MA-JIE/Reinforcement-Learning-MJ/blob/master/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/images/policy_evaluiation.png)<br>

策略改进
-----------
* 通过上面的多次迭代,我们计算出了一个给定策略下的价值函数,但我们希望寻找更好的策略以达到策略改进的目的.如果在某个状态s下,使用给定策略的动作,很明显计算结果就是该策略下的值函数v(s).但是,如果换成一个新的策略,即选择一个不同的动作,我们不知道会得到好的结果还是坏的结果.<br>
一种解决方法就是在状态s选择新的动作a后,继续遵循现有的策略 𝜋: <br>
![策略改进](https://github.com/MA-JIE/Reinforcement-Learning-MJ/blob/master/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/images/policy_improve1.png)<br>
但我们期望这样做可以让值函数更大.这里.引入了贪心策略的概念,即在每个状态选择能够使得q(s,a)最大的动作,形成一个贪心策略 𝜋', 满足:<br>
![策略改进](https://github.com/MA-JIE/Reinforcement-Learning-MJ/blob/master/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/images/policy_greedy.png)<br>


策略迭代
----------
* 通过上述的策略评估以及策略改进,我们就可以进行策略迭代,不断的求得新策略的值函数,在进行策略改进:<br>
![策略迭代](https://github.com/MA-JIE/Reinforcement-Learning-MJ/blob/master/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/images/policy_iteration.png)<br>
E:策略评估<br>
I:策略迭代<br>
一个有限的MDP必然只有有限中策略,所以在有限次的迭代后,我们一定可以找到一个最优的策略以及最优的价值函数.<br>
伪代码如下:<br>
![策略迭代](https://github.com/MA-JIE/Reinforcement-Learning-MJ/blob/master/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/images/policy_iteration_code.png)<br>


价值迭代
==========
在策略
