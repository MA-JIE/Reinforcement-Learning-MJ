# Social behavior for autonomous vehicles
关键字: Social Value Orientation(社会价值取向) | social compliance(社会责任感) | game theory(博弈理论) | inverse reinforcement learning(逆强化学习) <br>
博弈论:博弈中的回报函数是动态的，取决于汽车的状态以及环境信息，可以从人类司机的驾驶数据中学习回报函数(逆强化学习).
目前具有挑战性的场景: 并道以及无保护左转 <br>

# 主要贡献
(1) 将驾驶过程建模为动态博弈的过程并计算其纳什均衡状态 <br>
(2) 通过效用最大化来预测人类的动作 <br>
(3) 将SVO偏好整合进效用最大化的框架中 <br>
(4) 根据观测到的驾驶轨迹来估计SVO <br>
(5) 符合社会要求的自动驾驶行为的模拟 <br>
(6) 在NGSIM数据集上进行评估 <br>
# SVO
衡量智能体自私或者无私的程度，方便让我们预测智能体是如何与其他智能体进行交互与合作的 <br>
* 无人车如何估计SVO? <br>
对于人类来说，是从动作以及社会规则中观测和估计SVO的，我们通过确定与实际驾驶员轨迹最适合的预测轨迹的SVO来估计其他驾驶员的SVO,这样我们可以从轨迹数据中直接估计出SVO，拓展性高<br>
通过估计其他司机的SVO，我们可以制定无人车的控制策略. <br>
# Socially Compliant Driving
我们通过逆强化学习学习人类的回报函数，进而能够使得自动驾驶系统具有社会兼容性. <br>
为了完成无保护的转向动作，无人车首先通过传感器观测前方行驶而来的车辆的轨迹，通过从数据中学得的回报模型以及效用最大化模型，基于可能的SVO值，可以形成一些候选轨迹，最可能的SVO能够使得对应候选轨迹与真实轨迹最吻合，通过这个估计的SVO， 无人车将会形成未来的动作预测以及规划来保证安全转向. <br>
# Estimating Driver Behavior with SVO
对于一个亲社会的SVO，将会生成对应的轨迹1，一个自私的SVO，可能会生成完全不同的轨迹2，我们可以通过评估预测轨迹与真实轨迹距离上的高斯内核来计算候选SVO的似然，也可以使用最大熵模型，去建立基于距离的似然函数，我们使用这些方法从人类司机的轨迹去估计SVO. <br>

  




